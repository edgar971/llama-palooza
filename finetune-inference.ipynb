{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edgar-pino/.pyenv/versions/3.10.4/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import LlamaTokenizer\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = './models/7B-hf'\n",
    "fine_tuned_model_id = './llama-2/1691703585/final_checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.39s/it]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ./models/7B-hf and are newly initialized: ['model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tuned_model = AutoPeftModelForCausalLM.from_pretrained(fine_tuned_model_id, device_map='auto', quantization_config=nf4_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    }
   ],
   "source": [
    "model = tuned_model.to_bettertransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Implement the following program given the instructions:\n",
      "Write a simple program to calculate the softmax in python using just numpy.\n",
      "\n",
      "Code:\n",
      "Here's some sample python code which implements the softmax function using numpy:\n",
      "\n",
      "    import numpy as np\n",
      "    \n",
      "    def softmax(x):\n",
      "        \"\"\"Softmax function.\"\"\"\n",
      "        eps = np.finfo.eps * np.ones(x.shape)\n",
      "        return np.exp(x - eps) / np.sum(np.exp(x - eps))\n",
      "    \n",
      "    print(softmax([1, 2, 3]))\n",
      "    \n",
      "This code first imports the numpy library and defines a function called `softmax`, which takes an array of numbers as input and returns the softmax of those numbers. The function uses the `np.exp` and `np.sum` functions to calculate the exponential and sum of the exponentials of each element in the input array, respectively. Finally, the function divides the result by the sum of the exponentials to normalize the output. The output of the function is printed to the console.\n",
      "\n",
      "In this implementation, the epsilon value (`eps`) is used to prevent division by zero when the input array contains all zeros. This ensures that the output of the function is well-defined even when the input contains invalid values.\n",
      "\n",
      "---\n",
      "\n",
      "See also:\n",
      "\n",
      "- [NumPy](https://docs.scipy.org/doc/numpy/reference/index.html): A powerful N-dimensional array manipulation library for Python.\n",
      "- [Exp](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html): Returns the exponential of a number.\n",
      "- [Sum](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html): Returns the sum of the elements of an array.\n",
      "- [Divide](https://docs.scipy.org/doc/numpy/reference/generated/numpy.divide.html): Divides two arrays element-wise.\n",
      "- [Normalize](https://docs.scipy.org/doc/numpy/reference/generated/numpy.normalize.html): Normalizes a vector by dividing each element by its magnitude.\n",
      "\n",
      "---\n",
      "\n",
      "Related Article:\n",
      "\n",
      "- [How to implement a softmax function in Python?](https://towardsdatascience.com/how-to-implement-a-softmax-function-in-python-999\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "Implement the following program given the instructions:\n",
    "Write a simple program to calculate the softmax in python using just numpy.\n",
    "\n",
    "Code:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=512)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
